No arguments are passed. Using default.
*******************************************
INPUT DATASET: DESCRIBE, ORIGINAL
*******************************************
               toxic   severe_toxic        obscene         threat         insult  identity_hate
count  159571.000000  159571.000000  159571.000000  159571.000000  159571.000000  159571.000000
mean        0.095844       0.009996       0.052948       0.002996       0.049364       0.008805
std         0.294379       0.099477       0.223931       0.054650       0.216627       0.093420
min         0.000000       0.000000       0.000000       0.000000       0.000000       0.000000
25%         0.000000       0.000000       0.000000       0.000000       0.000000       0.000000
50%         0.000000       0.000000       0.000000       0.000000       0.000000       0.000000
75%         0.000000       0.000000       0.000000       0.000000       0.000000       0.000000
max         1.000000       1.000000       1.000000       1.000000       1.000000       1.000000
*******************************************
INPUT DATASET: HEAD
*******************************************
                 id                                       comment_text  toxic  severe_toxic  obscene  threat  insult  identity_hate
0  0000997932d777bf  Explanation\nWhy the edits made under my usern...      0             0        0       0       0              0
1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0             0        0       0       0              0
2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0             0        0       0       0              0
3  0001b41b1c6bb37e  "\nMore\nI can't make any real suggestions on ...      0             0        0       0       0              0
4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0             0        0       0       0              0
*******************************************
SCORE DATASET: DESCRIBE
*******************************************
              toxic  severe_toxic       obscene        threat        insult  identity_hate
count  63978.000000  63978.000000  63978.000000  63978.000000  63978.000000   63978.000000
mean       0.095189      0.005736      0.057692      0.003298      0.053565       0.011129
std        0.293478      0.075522      0.233161      0.057334      0.225160       0.104905
min        0.000000      0.000000      0.000000      0.000000      0.000000       0.000000
25%        0.000000      0.000000      0.000000      0.000000      0.000000       0.000000
50%        0.000000      0.000000      0.000000      0.000000      0.000000       0.000000
75%        0.000000      0.000000      0.000000      0.000000      0.000000       0.000000
max        1.000000      1.000000      1.000000      1.000000      1.000000       1.000000
*******************************************
SCORE DATASET: HEAD
*******************************************
                  id                                       comment_text  toxic  severe_toxic  obscene  threat  insult  identity_hate
5   0001ea8717f6de06  Thank you for understanding. I think very high...    0.0           0.0      0.0     0.0     0.0            0.0
7   000247e83dcc1211                   :Dear god this site is horrible.    0.0           0.0      0.0     0.0     0.0            0.0
11  0002f87b16116a7f  "::: Somebody will invariably try to add Relig...    0.0           0.0      0.0     0.0     0.0            0.0
13  0003e1cccfd5a40a  " \n\n It says it right there that it IS a typ...    0.0           0.0      0.0     0.0     0.0            0.0
14  00059ace3e3e9a53  " \n\n == Before adding a new product to the l...    0.0           0.0      0.0     0.0     0.0            0.0
Input lengths: mean = 394.0732213246768, std = 590.7202819048919, max = 5000
Score lengths: mean = 383.3793178905249, std = 598.1100714808687, max = 5000
Dimensions in training set: (127656, 8)
Dimensions in test set: (31915, 8)
Dimensions in score set: (63978, 8)
train_term_doc shape
(127656, 165909)
feature names sample (matrix columns, bag of words)
40028             cuprum17
105484     oktay_sinanoğlu
78205            jamnapari
69966        hizbolshaitan
39939              cumfart
1008                  1180
96419               mixins
116514           prokonsul
132038           shichidan
65362       group_projects
161529                xfer
51791            enforcing
71458                  hui
31012                  cew
46306          distasteful
137419            squardon
25205                bondi
36120         conditioning
17449          assaulthead
95142              midwakh
36635         consequences
9194            accessibly
158061           westwards
91107              mandodi
29631              carmina
45531            discarded
90850            malke2010
76073               interm
113247         plettenberg
45926             disloyal
98066              mouton1
9405            accustions
94438           messaggero
50335               eirikr
24052         blackpearll4
18575     authoritarianism
65514               gscode
165320                ساکی
120496                 re5
154669               ventu
58061            flattened
81740               kelkar
161109            wronghis
88178               loadee
96750              modesto
61965           geenrality
3807                   26d
100877        necrological
63188              girltab
63384              gleason
dtype: object
Creating model for  toxic
Mean accuracy over test set for toxic: 0.9624941250195833
Mean accuracy over score set for toxic: 0.9294132357998062
Confusion matrix over test set for toxic:
[[28589   278]
 [  919  2129]]
Confusion matrix over score set for toxic:
[[54753  3135]
 [ 1381  4709]]
Saving model to ./Models/toxic.pkl
Creating model for  severe_toxic
Mean accuracy over test set for severe_toxic: 0.990224032586558
Mean accuracy over score set for severe_toxic: 0.9928881803119822
Confusion matrix over test set for severe_toxic:
[[31491    92]
 [  220   112]]
Confusion matrix over score set for severe_toxic:
[[63386   225]
 [  230   137]]
Saving model to ./Models/severe_toxic.pkl
Creating model for  obscene
Mean accuracy over test set for obscene: 0.9802913990286699
Mean accuracy over score set for obscene: 0.9652380505798869
Confusion matrix over test set for obscene:
[[30083   185]
 [  444  1203]]
Confusion matrix over score set for obscene:
[[59219  1068]
 [ 1156  2535]]
Saving model to ./Models/obscene.pkl
Creating model for  threat
Mean accuracy over test set for threat: 0.9972740090866363
Mean accuracy over score set for threat: 0.9967801431742161
Confusion matrix over test set for threat:
[[31791    17]
 [   70    37]]
Confusion matrix over score set for threat:
[[63713    54]
 [  152    59]]
Saving model to ./Models/threat.pkl
Creating model for  insult
Mean accuracy over test set for insult: 0.9716120946263512
Mean accuracy over score set for insult: 0.9637531651505205
Confusion matrix over test set for insult:
[[30119   244]
 [  662   890]]
Confusion matrix over score set for insult:
[[59784   767]
 [ 1552  1875]]
Saving model to ./Models/insult.pkl
Creating model for  identity_hate
Mean accuracy over test set for identity_hate: 0.9924173586088046
Mean accuracy over score set for identity_hate: 0.9902153865391228
Confusion matrix over test set for identity_hate:
[[31609    43]
 [  199    64]]
Confusion matrix over score set for identity_hate:
[[63144   122]
 [  504   208]]
Saving model to ./Models/identity_hate.pkl
